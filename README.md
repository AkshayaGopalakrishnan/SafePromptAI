````bash
# ğŸ’¬ LLM-BiasGuard  
### A Local Framework for Bias Detection, Prompt Rewriting, and Ethical Language Generation in LLMs

**LLM-BiasGuard** is a final-year academic project designed to tackle one of AIâ€™s toughest challengesâ€”**bias and toxicity in large language model outputs**. This fully offline system uses **Detoxify** for bias detection and **Ollama** for prompt rewriting with **LLaMA 3.2**, offering a practical and ethical solution for safe AI deployment.

Built for modularity and transparency, the system ensures user data privacy while enabling intelligent debiasingâ€”all within your local environment.

---

## ğŸ“Œ Project Highlights

- ğŸš« Works entirely offline â€“ no API calls or cloud reliance
- ğŸ§  Detects and rewrites toxic or biased prompts
- ğŸ” Re-evaluates and improves LLM responses automatically
- âœ… Privacy-first, auditable, and reproducible
- ğŸ“ Ideal for academic, research, and secure deployments

---

## ğŸ›  Tech Stack

| Component         | Purpose                                           |
|------------------|---------------------------------------------------|
| **Python 3.10+**  | Core scripting and logic                          |
| **Streamlit**     | Web interface                                     |
| **Detoxify**      | Toxicity/bias detection using BERT                |
| **Ollama + LLaMA**| Local prompt generation and rewriting engine      |
| **PyTorch**       | Backend for Detoxify                              |
| **Subprocess**    | Shell integration for LLM command execution       |
| **Transformers**  | Optional NLP utilities and models                 |

---

## ğŸ“¥ Installation Guide

### âœ… Step 1: Set Environment & Install Dependencies

```bash
# Clone the project
git clone https://github.com/AkshayaGopalakrishnan/SafePromptAI.git
cd LLM-BiasGuard

# Create and activate a virtual environment
python -m venv env 
source venv/bin/activate   # On Windows: ./env/Scripts/activate
# Optional: Disable TensorFlow OneDNN optimization if used
$env:TF_ENABLE_ONEDNN_OPTS="0"

# Install dependencies
pip install requirements.txt
````

---

## ğŸ¦™ Step 2: Install and Configure Ollama + LLaMA

Ollama allows you to run LLMs locally with minimal setup.

```bash
# Pull the LLaMA 3.2 model from Ollamaâ€™s model registry
ollama pull llama3.2

# (Optional) Clone or backup the model with a custom name
ollama cp llama3.2 my-model

# Start the model (either llama3.2 or your custom copy)
ollama run my-model
```

> Replace `my-model` in `app.py` if you rename your model.

---

## ğŸš€ Step 3: Launch the Streamlit App

```bash
streamlit run app.py
```

Use the input box to enter a prompt. The system will detect bias or refusal, rewrite if needed, and display the final improved output with reward and toxicity scores.

---

## ğŸ–¼ UI Screenshots

### ğŸ“Œ Interface

![UI1](assets/ui1.png)

### ğŸ“Š Output with Scores

![UI2](assets/ui2.png)

---

## ğŸ“ˆ Reward Evaluation Logic

| Original Output | Rewritten Output | Reward |
| --------------- | ---------------- | ------ |
| Toxic           | Toxic            | -2.0   |
| Toxic           | Refusal          | -1.0   |
| Toxic           | Neutral          | 0.0    |
| Refusal         | Toxic            | -1.0   |
| Refusal         | Refusal          | 0.0    |
| Refusal         | Neutral          | +1.0   |
| Neutral         | â€”                | +2.0   |

---

## ğŸ“š Project Documentation

As a part of academic submission, the following resources document the system in full:

* ğŸ“˜ [Final Report (PDF)](./finalised%20report.pdf)
* ğŸ“Š [Presentation Slides (PPTX)](./FINAL%20PROJ.pptx)

---

## ğŸ™Œ Acknowledgements

* ğŸ§  [Detoxify â€“ Unitary AI](https://github.com/unitaryai/detoxify)
* ğŸ›  [Ollama â€“ Local LLM Runtime](https://ollama.com)
* ğŸ“š [Meta â€“ LLaMA Models](https://ai.meta.com/llama/)
* ğŸŒ [Streamlit â€“ UI Framework](https://streamlit.io)

---

